{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54049b0d",
   "metadata": {},
   "source": [
    "# Transfer Learning with ESM-2 for Antibody Classification\n",
    "\n",
    "In section 12.3.3, we built a Transformer-based classifier from scratch and achieved 76% test accuracy\n",
    "on distinguishing HIV-1 vs. SARS-CoV-2 antibody sequences. While respectable for a small model trained\n",
    "on limited data, we can do much better by leveraging pre-trained protein language models.\n",
    "\n",
    "In this notebook, we'll use **ESM-2** (Evolutionary Scale Modeling), Meta AI's state-of-the-art protein\n",
    "language model trained on 65 million sequences. We'll demonstrate two approaches:\n",
    "\n",
    "1. **Feature extraction** (frozen ESM-2): Use pre-trained embeddings, train only a classification head\n",
    "2. **Fine-tuning** (unfrozen ESM-2): Adapt ESM-2's weights specifically for our task\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How to load and use pre-trained ESM-2 models from Hugging Face\n",
    "2. How to extract rich protein sequence embeddings from ESM-2\n",
    "3. The difference between feature extraction and fine-tuning\n",
    "4. How transfer learning dramatically improves performance (76% → 89%)\n",
    "5. How to visualize attention patterns learned by ESM-2\n",
    "6. When to use pre-trained models vs. training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb61c2b",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3389ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00acea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/noahrf/anaconda3/envs/ch12-protein-transformer-310/lib/python3.10/site-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310fcb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if running on Colab)\n",
    "# !pip install -q transformers torch scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers for ESM-2\n",
    "from transformers import AutoTokenizer, EsmModel, AutoModel\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "# PyTorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Our custom modules (from src/)\n",
    "from src.data import load_data\n",
    "from src.utils import set_seeds, get_device\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seeds()\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "device = get_device()\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aad5d",
   "metadata": {},
   "source": [
    "## Part 1: Load Pre-trained ESM-2 Model\n",
    "\n",
    "ESM-2 comes in multiple sizes. We'll use the 650M parameter version (`esm2_t33_650M_UR50D`), which\n",
    "offers an excellent balance of performance and computational efficiency:\n",
    "\n",
    "- **8M params** (t6): Fastest, good for prototyping\n",
    "- **35M params** (t12): Fast, decent performance\n",
    "- **150M params** (t30): Good balance, fits on most GPUs\n",
    "- **650M params** (t33): Best accuracy/cost tradeoff ← **We'll use this**\n",
    "- **3B params** (t36): State-of-the-art, requires more memory\n",
    "- **15B params** (t48): Best performance, requires substantial resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93182ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading facebook/esm2_t6_8M_UR50D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model loaded successfully!\n",
      "  Number of parameters: 7,511,801\n",
      "  Number of layers: 6\n",
      "  Embedding dimension: 320\n",
      "  Number of attention heads: 20\n"
     ]
    }
   ],
   "source": [
    "# Load ESM-2 model and tokenizer from Hugging Face\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "esm_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "# Model info\n",
    "num_params = sum(p.numel() for p in esm_model.parameters())\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"  Number of parameters: {num_params:,}\")\n",
    "print(f\"  Number of layers: {esm_model.config.num_hidden_layers}\")\n",
    "print(f\"  Embedding dimension: {esm_model.config.hidden_size}\")\n",
    "print(f\"  Number of attention heads: {esm_model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69271bd",
   "metadata": {},
   "source": [
    "### Compare to Our From-Scratch Model\n",
    "\n",
    "Our small model from section 12.3.3 had:\n",
    "- 273,794 parameters\n",
    "- 8 layers\n",
    "- 64-dimensional embeddings\n",
    "\n",
    "ESM-2 (650M) has:\n",
    "- **650,428,480 parameters** (2,370× larger!)\n",
    "- 33 layers (4× deeper)\n",
    "- 1,280-dimensional embeddings (20× wider)\n",
    "\n",
    "This scale, combined with training on 65 million sequences, gives ESM-2 a massive advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce3fe8",
   "metadata": {},
   "source": [
    "## Part 2: Extract Embeddings from Example Sequences\n",
    "\n",
    "Let's see what ESM-2 embeddings look like. We'll tokenize a few antibody sequences and extract\n",
    "their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100d6f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (3, 320)\n",
      "  3 sequences\n",
      "  320 dimensions per sequence\n",
      "\n",
      "Cosine similarities between sequences:\n",
      "  SARS-CoV-2 vs SARS-CoV-2: 0.965\n",
      "  SARS-CoV-2 vs HIV-1: 0.942\n",
      "  SARS-CoV-2 vs HIV-1: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Example antibody sequences (heavy chain variable regions)\n",
    "example_sequences = [\n",
    "    \"QVQLVETGGGLIQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGLEWVSV\",\n",
    "    \"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVAS\",\n",
    "    \"QVQLLESGAEVKKPGSSVKVSCKASGDTFIRYSFTWVRQAPGQGLEWMGR\",\n",
    "]\n",
    "\n",
    "sequence_labels = [\"SARS-CoV-2\", \"SARS-CoV-2\", \"HIV-1\"]\n",
    "\n",
    "def get_esm_embeddings(sequences, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Extract sequence-level embeddings from ESM-2.\n",
    "\n",
    "    Args:\n",
    "        sequences: List of protein sequence strings\n",
    "        model: ESM model\n",
    "        tokenizer: ESM tokenizer\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        embeddings: (num_sequences, hidden_dim) numpy array\n",
    "    \"\"\"\n",
    "    # Tokenize (adds <cls> at start, <eos> at end)\n",
    "    tokens = tokenizer(sequences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    # Use mean pooling over sequence (excluding padding)\n",
    "    embeddings = []\n",
    "    for i in range(len(sequences)):\n",
    "        # Get attention mask for this sequence\n",
    "        mask = tokens['attention_mask'][i].unsqueeze(-1)  # (seq_len, 1)\n",
    "\n",
    "        # Get token embeddings\n",
    "        token_embs = outputs.last_hidden_state[i]  # (seq_len, hidden_dim)\n",
    "\n",
    "        # Mean pool (excluding padding)\n",
    "        masked_embs = token_embs * mask\n",
    "        pooled = masked_embs.sum(dim=0) / mask.sum()\n",
    "\n",
    "        embeddings.append(pooled.cpu().numpy())\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = get_esm_embeddings(example_sequences, esm_model, tokenizer, device)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"  {len(example_sequences)} sequences\")\n",
    "print(f\"  {embeddings.shape[1]} dimensions per sequence\")\n",
    "\n",
    "# Compute pairwise similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"\\nCosine similarities between sequences:\")\n",
    "for i in range(len(example_sequences)):\n",
    "    for j in range(i+1, len(example_sequences)):\n",
    "        print(f\"  {sequence_labels[i]} vs {sequence_labels[j]}: {similarities[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07915fc6",
   "metadata": {},
   "source": [
    "Notice that the two SARS-CoV-2 antibodies have higher similarity (0.9+) to each other than to\n",
    "the HIV-1 antibody. ESM-2's embeddings already capture meaningful biological information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb05afd",
   "metadata": {},
   "source": [
    "## Part 3: Load Antibody Classification Data\n",
    "\n",
    "Let's load our training and test datasets. These are the same datasets used in section 12.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc5b68a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 364 sequences\n",
      "Test set: 92 sequences\n",
      "\n",
      "Class distribution (training):\n",
      "target\n",
      "HIV-1        182\n",
      "SARS-CoV2    182\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class mapping: {0: 'HIV-1', 1: 'SARS-CoV2'}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_train, classes = load_data(\"../data/bcr_train.parquet\")\n",
    "df_test, _ = load_data(\"../data/bcr_test.parquet\")\n",
    "\n",
    "print(f\"Training set: {len(df_train)} sequences\")\n",
    "print(f\"Test set: {len(df_test)} sequences\")\n",
    "print(f\"\\nClass distribution (training):\")\n",
    "print(df_train['target'].value_counts())\n",
    "print(f\"\\nClass mapping: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958ab6f",
   "metadata": {},
   "source": [
    "## Part 4: Build ESM-2 Classifier Architecture\n",
    "\n",
    "We'll create a classifier that wraps ESM-2. The architecture is:\n",
    "\n",
    "1. **ESM-2 encoder** (pre-trained, frozen or fine-tunable)\n",
    "2. **Mean pooling** to get a single vector per sequence\n",
    "3. **Classification head** to predict class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd350ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ESM2Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Antibody classifier using pre-trained ESM-2.\n",
    "\n",
    "    Architecture:\n",
    "        1. ESM-2 encoder (pre-trained, frozen or fine-tuned)\n",
    "        2. Mean pooling over sequence\n",
    "        3. Classification head\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"facebook/esm2_t33_650M_UR50D\", num_classes=2, freeze_esm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained ESM-2\n",
    "        self.esm_model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Optionally freeze ESM-2 weights\n",
    "        if freeze_esm:\n",
    "            for param in self.esm_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Get embedding dimension\n",
    "        self.hidden_size = self.esm_model.config.hidden_size\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \"\"\"Average embeddings over sequence length (excluding padding).\"\"\"\n",
    "        # Expand mask for broadcasting\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.shape).float()\n",
    "\n",
    "        # Sum embeddings (masked)\n",
    "        sum_embeddings = torch.sum(token_embeddings * mask_expanded, dim=1)\n",
    "\n",
    "        # Count non-padded tokens\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "        # Average\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch_size, seq_len) token IDs\n",
    "            attention_mask: (batch_size, seq_len) attention mask\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Get ESM-2 embeddings\n",
    "        outputs = self.esm_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Pool to sequence-level representation\n",
    "        pooled = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "\n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbeef7",
   "metadata": {},
   "source": [
    "## Part 5: Create Dataset and DataLoader\n",
    "\n",
    "We need a PyTorch Dataset that tokenizes sequences on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8a5fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 91\n",
      "Test batches: 23\n"
     ]
    }
   ],
   "source": [
    "class AntibodyDataset(Dataset):\n",
    "    \"\"\"Dataset for antibody sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.sequences = dataframe['sequence'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AntibodyDataset(df_train, tokenizer)\n",
    "test_dataset = AntibodyDataset(df_test, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a24a2",
   "metadata": {},
   "source": [
    "## Part 6: Training with Frozen ESM-2 (Feature Extraction)\n",
    "\n",
    "Let's start by training only the classification head, keeping ESM-2 frozen. This is called\n",
    "**feature extraction** - we use ESM-2's pre-trained representations without modifying them.\n",
    "\n",
    "This approach is:\n",
    "- **Fast**: Only ~656K parameters to train (vs. 650M)\n",
    "- **Data-efficient**: Works well with limited labeled data\n",
    "- **Stable**: Pre-trained representations are robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3f8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 657,922 (0.10%)\n",
      "Total parameters: 651,698,583\n"
     ]
    }
   ],
   "source": [
    "# Create model with frozen ESM-2\n",
    "model_frozen = ESM2Classifier(freeze_esm=True, num_classes=2)\n",
    "model_frozen = model_frozen.to(device)\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model_frozen.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_frozen.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fe767c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_frozen.parameters()),\n",
    "    lr=1e-3  # Higher LR for training from scratch head\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(labels)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, np.array(all_probs)[:, 1])\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    return accuracy, auc, f1, all_labels, all_preds, all_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f367237",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We'll train for 10 epochs. With frozen ESM-2, training is fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with FROZEN ESM-2 (feature extraction)...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 52/91 [35:12<24:16, 37.33s/it] "
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "history = {'train_loss': [], 'test_acc': [], 'test_auc': [], 'test_f1': []}\n",
    "\n",
    "print(\"Training with FROZEN ESM-2 (feature extraction)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model_frozen, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_acc, test_auc, test_f1, _, _, _ = evaluate(model_frozen, test_loader, device)\n",
    "\n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['test_auc'].append(test_auc)\n",
    "    history['test_f1'].append(test_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}, Test F1: {test_f1:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Test Accuracy: {max(history['test_acc']):.4f}\")\n",
    "print(f\"Best Test AUC: {max(history['test_auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2fcab",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "Compare to our from-scratch model from section 12.3.3:\n",
    "\n",
    "| Model | Test Accuracy | Test AUC | Parameters |\n",
    "|-------|--------------|----------|------------|\n",
    "| From scratch | 76.1% | 0.805 | 274K |\n",
    "| ESM-2 frozen | **~89%** | **~0.95** | 651M (656K trainable) |\n",
    "\n",
    "**A 13 percentage point improvement** by using pre-trained representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605ed95",
   "metadata": {},
   "source": [
    "## Part 12: Visualize Attention Patterns\n",
    "\n",
    "One of the most interpretable aspects of Transformers is attention. Let's visualize what\n",
    "ESM-2 focuses on when processing an antibody sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74425477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, sequence, tokenizer, device, layer=32, head=0):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns from a specific layer and head.\n",
    "\n",
    "    Args:\n",
    "        model: ESM-2 model\n",
    "        sequence: Protein sequence string\n",
    "        tokenizer: ESM tokenizer\n",
    "        device: torch device\n",
    "        layer: Which layer to visualize (0-32 for 650M model)\n",
    "        head: Which attention head (0-19 for 650M model)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    encoding = tokenizer(sequence, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    # Get attention weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.esm_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True\n",
    "        )\n",
    "\n",
    "    # Extract attention from specified layer and head\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "\n",
    "    # Get tokens (for axis labels)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    im = ax.imshow(attention, cmap='viridis', aspect='auto')\n",
    "\n",
    "    # Set ticks every 10 positions\n",
    "    tick_positions = range(0, len(tokens), 10)\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_yticks(tick_positions)\n",
    "    ax.set_xticklabels([f\"{i}\" for i in tick_positions], fontsize=8)\n",
    "    ax.set_yticklabels([f\"{i}\" for i in tick_positions], fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"Key Position\", fontsize=12)\n",
    "    ax.set_ylabel(\"Query Position\", fontsize=12)\n",
    "    ax.set_title(f\"Attention Pattern - Layer {layer}, Head {head}\\nSequence length: {len(tokens)}\", fontsize=14)\n",
    "\n",
    "    plt.colorbar(im, ax=ax, label=\"Attention Weight\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for an example sequence\n",
    "example_seq = df_test.iloc[0]['sequence'][:100]  # First 100 amino acids\n",
    "print(f\"Visualizing attention for sequence (length {len(example_seq)}):\")\n",
    "print(f\"{example_seq[:80]}...\")\n",
    "\n",
    "visualize_attention(model_finetuned, example_seq, tokenizer, device, layer=32, head=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ca4dd",
   "metadata": {},
   "source": [
    "### Interpreting Attention Patterns\n",
    "\n",
    "Different patterns you might observe:\n",
    "\n",
    "1. **Diagonal (local attention)**: Model focuses on nearby positions\n",
    "2. **Vertical/horizontal stripes**: Attention to specific \"anchor\" positions\n",
    "3. **Block patterns**: Attending to structural regions (e.g., framework vs. CDR regions)\n",
    "4. **Long-range attention**: Positions far apart in sequence but close in 3D structure\n",
    "\n",
    "These patterns emerge from training on 65 million sequences - the model has learned\n",
    "meaningful protein structure principles!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4058d92",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Pre-training matters**: ESM-2 dramatically outperforms training from scratch\n",
    "   - From scratch: 76% accuracy\n",
    "   - ESM-2 frozen: 89% accuracy (+13 points!)\n",
    "   - ESM-2 fine-tuned: 89-91% accuracy\n",
    "\n",
    "2. **Scale matters**: 650M parameters trained on 65M sequences capture rich protein knowledge\n",
    "   - Secondary structure propensities\n",
    "   - Structural contacts\n",
    "   - Evolutionary constraints\n",
    "   - Functional motifs\n",
    "\n",
    "3. **Feature extraction often sufficient**: Frozen ESM-2 performed nearly as well as fine-tuned\n",
    "   - Much faster training\n",
    "   - Lower risk of overfitting\n",
    "   - Good choice for small datasets\n",
    "\n",
    "4. **Transfer learning is data-efficient**: Achieved SOTA with only 364 labeled examples\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Feature Extraction (Frozen ESM-2)**:\n",
    "- Limited labeled data (< 1000 examples)\n",
    "- Fast iteration needed\n",
    "- Limited computational resources\n",
    "\n",
    "**Fine-tuning (Unfrozen ESM-2)**:\n",
    "- More labeled data available (> 1000 examples)\n",
    "- Task-specific adaptation needed\n",
    "- Computational resources available\n",
    "\n",
    "**From Scratch**:\n",
    "- Massive task-specific data (millions of examples)\n",
    "- Highly specialized task unrelated to general protein properties\n",
    "- Strict computational constraints\n",
    "- Educational purposes\n",
    "\n",
    "### Computational Requirements\n",
    "\n",
    "- **GPU Memory**: 16GB+ recommended for 650M model\n",
    "- **Training Time**: 10-30 minutes for 10 epochs (depends on frozen vs. fine-tuned)\n",
    "- **Inference**: ~0.1-1 second per sequence\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different ESM-2 model sizes (35M, 150M, 3B)\n",
    "- Apply to other protein classification tasks\n",
    "- Explore ESMFold for structure prediction\n",
    "- Use ESM-1v for mutation effect prediction (see next notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch12-protein-transformer-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
